# ğŸ›³ Titanic Survival Prediction with Optuna

## í”„ë¡œì íŠ¸ ê°œìš”
- **ëª©ì **: íƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡ (Titanic: Machine Learning from Disaster)
- **ì£¼ìš” ë‚´ìš©**: ë¨¸ì‹ ëŸ¬ë‹ ì „ì²˜ë¦¬~ëª¨ë¸ë§~Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ê¹Œì§€ ì‹¤ìŠµ
- **ë°ì´í„°**: titanic.csv

## ì£¼ìš” ê¸°ëŠ¥ ë° ì½”ë“œ

### 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
```python
import pandas as pd

df = pd.read_csv("titanic.csv")
df["Age"] = df["Age"].fillna(df["Age"].mean())
df["Embarked"] = df["Embarked"].fillna("S")
# ë²”ì£¼í˜• ë³€ìˆ˜ ì›-í•« ì¸ì½”ë”©
X = pd.get_dummies(df[["Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked"]])
y = df["Survived"]
```

### 2. ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§
```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True)
scaler = RobustScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
```

### 3. ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

models = {
    'Logistic Regression': LogisticRegression(),
    'Random Forest': RandomForestClassifier(),
    'XGBoost': XGBClassifier()
}
for name, model in models.items():
    model.fit(X_train, y_train)
    pred = model.predict(X_valid)
    print(f"{name} Accuracy: {accuracy_score(y_valid, pred):.4f}")
```

### 4. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì˜ˆì‹œ
```python
import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

def objective(trial):
    n_estimators = trial.suggest_int('n_estimators', 50, 200)
    max_depth = trial.suggest_int('max_depth', 2, 10)
    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)
    clf.fit(X_train, y_train)
    pred = clf.predict(X_valid)
    return accuracy_score(y_valid, pred)

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=30)
print("Best params:", study.best_params)
print("Best accuracy:", study.best_value)
```

### 5. ê²°ê³¼ ìš”ì•½
- Optuna íŠœë‹ í›„ ìƒìœ„ ëª¨ë¸ ì„±ëŠ¥:
  - Logistic Regression: 78.5%
  - Random Forest (Optuna): 82.1%
  - XGBoost (Optuna): 83.4%
  - XGBoost + StratifiedKFold + íŠœë‹: 85% ì´ìƒ

---

## ì°¸ê³ /ì‹¤í–‰ ë°©ë²•
- Jupyterì—ì„œ `ML_íƒ€ì´íƒ€ë‹‰__Optuna.ipynb`, `ML_íƒ€ì´íƒ€ë‹‰__Optuna_í•˜ì´í¼íŒŒë¼ë¯¸í„°íŠœë‹.ipynb` ì‹¤í–‰
- ë°ì´í„°: titanic.csv

## ì£¼ìš” ì½”ë“œ/ë…¸íŠ¸ë¶
- [ML_íƒ€ì´íƒ€ë‹‰__Optuna.ipynb](./ML_íƒ€ì´íƒ€ë‹‰__Optuna.ipynb)
- [ML_íƒ€ì´íƒ€ë‹‰__Optuna_í•˜ì´í¼íŒŒë¼ë¯¸í„°íŠœë‹.ipynb](./ML_íƒ€ì´íƒ€ë‹‰__Optuna_í•˜ì´í¼íŒŒë¼ë¯¸í„°íŠœë‹.ipynb)















