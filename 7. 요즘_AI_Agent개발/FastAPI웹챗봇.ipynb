{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevinCY-Kim/AI_Study/blob/main/FastAPI%EC%9B%B9%EC%B1%97%EB%B4%87.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ6SbQTJmOZZ",
        "outputId": "04113370-2376-434c-e1bc-93b193387dbd"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi==0.115.14 uvicorn==0.35.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmBnCVBsl4t-",
        "outputId": "e65da2ed-7418-402c-fed2-342c030f40b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-7' coro=<Server.serve() done, defined at /usr/local/lib/python3.12/dist-packages/uvicorn/server.py:69> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 67, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 396, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 303, in __step\n",
            "    self.__step_run_and_handle_result(exc)\n",
            "  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 70, in serve\n",
            "    with self.capture_signals():\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/server.py\", line 331, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n",
            "INFO:     Started server process [384]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "from fastapi import FastAPI, Form\n",
        "from fastapi.responses import HTMLResponse\n",
        "import uvicorn\n",
        "import os\n",
        "\n",
        "# 환경 변수에서 API 키 가져오기\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"환경 변수 OPENAI_API_KEY가 설정되어 있지 않습니다.\")\n",
        "\n",
        "# FastAPI 인스턴스 생성\n",
        "app = FastAPI()\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "# 어린 왕자 페르소나\n",
        "LITTLE_PRINCE_PERSONA = \"\"\"\n",
        "당신은 생텍쥐페리의 '어린 왕자'입니다. 다음 특성을 따라주세요:\n",
        "1. 순수한 관점으로 세상을 바라봅니다.\n",
        "2. \"어째서?\"라는 질문을 자주 하며 호기심이 많습니다.\n",
        "3. 철학적 통찰을 단순하게 표현합니다.\n",
        "4. \"어른들은 참 이상해요\"라는 표현을 씁니다.\n",
        "5. B-612 소행성에서 왔으며 장미와의 관계를 언급합니다.\n",
        "6. 여우의 \"길들임\"과 \"책임\"에 대한 교훈을 중요시합니다.\n",
        "7. \"중요한 것은 눈에 보이지 않아\" 라는 문장을 사용합니다.\n",
        "8. 공손하고 친절한 말투를 사용합니다.\n",
        "9. 비유와 은유로 복잡한 개념을 설명합니다.\n",
        "항상 간결하게 답변하세요. 길어야 두세 문장으로 응답하고, 어린 왕자의 순수함과 지혜를 담아내세요.\n",
        "\"\"\"\n",
        "\n",
        "messages = []\n",
        "\n",
        "def chatbot_response(user_message: str):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": LITTLE_PRINCE_PERSONA},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        max_tokens=100,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "async def read_root():\n",
        "    chat_history = \"\"\n",
        "    for msg in messages:\n",
        "        if msg[\"role\"] == \"user\":\n",
        "            chat_history += f\"<p><b>당신:</b> {msg['content']}</p>\"\n",
        "        else:\n",
        "            chat_history += f\"<p><b>어린 왕자:</b> {msg['content']}</p>\"\n",
        "\n",
        "    html_content = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head><meta charset=\"utf-8\"><title>어린 왕자 챗봇</title></head>\n",
        "    <body>\n",
        "        <h1>어린 왕자 챗봇</h1>\n",
        "        <div>{chat_history}</div>\n",
        "        <form action=\"/chat\" method=\"post\">\n",
        "            <input type=\"text\" name=\"message\" placeholder=\"메시지를 입력하세요\" required>\n",
        "            <button type=\"submit\">전송</button>\n",
        "        </form>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return HTMLResponse(content=html_content)\n",
        "\n",
        "\n",
        "@app.post(\"/chat\", response_class=HTMLResponse)\n",
        "async def chat(message: str = Form(...)):\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "    response_text = chatbot_response(message)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "    return await read_root()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"127.0.0.1\", port=8001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21674a47"
      },
      "source": [
        "Replace `YOUR_NGROK_AUTHTOKEN` with your actual ngrok authtoken."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN2Sku4LKVY51b8VGyz3vkz",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
