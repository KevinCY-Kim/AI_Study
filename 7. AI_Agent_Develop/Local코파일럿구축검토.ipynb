{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b925ff1",
   "metadata": {},
   "source": [
    "## Local 코파일럿 구축\n",
    "### 로컬 LLM 코딩 환경 구성 예시\n",
    "- Ollama : 모델 관리 및 로컬 LLM 실행\n",
    "- LM Studio : VSCode 연결용 인터페이스\n",
    "- FastAPI : LLM 백엔드 서버로 래핑\n",
    "- VSCode Agent API : 명령형 인터랙션\n",
    "\n",
    "### API 호출 없이 로컬 추론만으로 코드 생성, 수정, 문맥 대화가 가능\n",
    "-   구분\t         :                 내용\n",
    "-  필요 최소 GPU : RTX 4070 이상 (12GB VRAM)\n",
    "-  Finetuning까지 포함 시 : RTX 4090 + 64GB RAM\n",
    "-  실질적 결과물 :           GPT-3.5 수준 코드 어시스턴트 구현 가능\n",
    "-  비용 구조 :              OpenAI 호출비 0원 / 전력비만 소모\n",
    "-  운용 형태 :              Ollama + LM Studio + VSCode Agent 연동형\n",
    "\n",
    "### 폐쇄망 + 로컬 LLM(GPT-3.5 수준)\n",
    "- 용도: 학습, 연구, 실험, PoC(Proof of Concept)\n",
    "- 장점: 외부 API 호출 필요 없고, 데이터 프라이버시 완벽, 모델 구조 이해 및 Finetuning 실험 가능\n",
    "- 단점: 실제 개발 생산성 측면에서는 최신 Copilot/GPT-4 대비 제한적, 최신 라이브러리/언어 문법 반영 어렵고, 코드 추천/디버깅 품질 낮음"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
