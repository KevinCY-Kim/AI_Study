---
base_model: unsloth/llama-3.2-1b-bnb-4bit
library_name: transformers
model_name: outputs
tags:
- generated_from_trainer
- sft
- unsloth
- trl
licence: license
---

# π·οΈ outputs: unsloth κΈ°λ° LLM νμΈνλ‹ κ²°κ³Ό

## κ°μ”
- **λ©μ **: unsloth/llama-3.2-1b-bnb-4bit κΈ°λ° LLM νμΈνλ‹ κ²°κ³Ό λ° μ‹¤ν— κ΄€λ¦¬
- **μ£Όμ” λ‚΄μ©**: SFT(μ§€λ„ νμΈνλ‹), LoRA, PEFT, μ‹¤ν— ν™κ²½/ν•μ΄νΌνλΌλ―Έν„°/κ²°κ³Ό/λ¨λΈ μΉ΄λ“/μ¶”λ΅  μμ‹ λ“±

## ν€µμ¤νƒ€νΈ (μ¶”λ΅  μμ‹)
```python
from transformers import pipeline
question = "If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?"
generator = pipeline("text-generation", model="outputs", device="cuda")
output = generator([{"role": "user", "content": question}], max_new_tokens=128, return_full_text=False)[0]
print(output["generated_text"])
```

## μ£Όμ” νμΌ/κµ¬μ„±
- adapter_model.safetensors : νμΈνλ‹λ LoRA/PEFT μ–΄λ‘ν„° κ°€μ¤‘μΉ
- adapter_config.json : PEFT/LoRA μ„¤μ • (target_modules, lora_alpha, r λ“±)
- tokenizer_config.json, tokenizer.json : ν† ν¬λ‚μ΄μ € μ •λ³΄
- trainer_state.json, training_args.bin : ν•™μµ μƒνƒ, ν•μ΄νΌνλΌλ―Έν„°
- README.md : λ¨λΈ μΉ΄λ“, μ‹¤ν— ν™κ²½, ν™μ©λ²•

## μ‹¤ν— ν™κ²½/ν”„λ μ„μ›ν¬
- TRL: 0.22.2
- Transformers: 4.56.0
- Pytorch: 2.8.0+cu126
- Datasets: 3.6.0
- Tokenizers: 0.22.0

## μ£Όμ” ν•μ΄νΌνλΌλ―Έν„° (adapter_config.json)
- base_model_name_or_path: unsloth/llama-3.2-1b-bnb-4bit
- lora_alpha: 16, r: 16, target_modules: [q_proj, k_proj, o_proj, gate_proj, v_proj, down_proj, up_proj]
- task_type: CAUSAL_LM
- inference_mode: true

## μ‹¤ν— κ²°κ³Ό/ν™μ©
- outputs/checkpoint-13/ : κ° epochλ³„ μ²΄ν¬ν¬μΈνΈ, safetensors, config, ν† ν¬λ‚μ΄μ €, ν•™μµ μƒνƒ, README λ“±
- wandb/ : μ‹¤ν— λ΅κ·Έ, ν•μ΄νΌνλΌλ―Έν„°, μ„±λ¥ μ¶”μ΄, μ²΄ν¬ν¬μΈνΈ κ΄€λ¦¬

## μ°Έκ³ /μ‹¤ν–‰ λ°©λ²•
- outputs/checkpoint-13/README.md, wandb/ μ°Έκ³ 
- νμΈνλ‹λ λ¨λΈλ΅ pipeline μ¶”λ΅ , LoRA/PEFT μ–΄λ‘ν„° ν™μ© κ°€λ¥

## κ΄€λ ¨ λ§ν¬
- [outputs/checkpoint-13/README.md](./checkpoint-13/README.md)
- [wandb/](../../wandb/)