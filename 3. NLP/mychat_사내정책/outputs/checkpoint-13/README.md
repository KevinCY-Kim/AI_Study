---
base_model: unsloth/llama-3.2-1b-bnb-4bit
library_name: peft
pipeline_tag: text-generation
tags:
- base_model:adapter:unsloth/llama-3.2-1b-bnb-4bit
- lora
- sft
- transformers
- trl
- unsloth
---

# π§© outputs/checkpoint-13: LLM νμΈνλ‹ μ²΄ν¬ν¬μΈνΈ μƒμ„Έ

## κ°μ”
- **λ©μ **: unsloth κΈ°λ° LLM νμΈνλ‹ μ‹¤ν—μ epoch 13 μ²΄ν¬ν¬μΈνΈ κ²°κ³Ό
- **μ£Όμ” λ‚΄μ©**: LoRA/PEFT μ–΄λ‘ν„°, config, ν† ν¬λ‚μ΄μ €, ν•™μµ μƒνƒ, ν•μ΄νΌνλΌλ―Έν„°, μ‹¤ν— ν™κ²½, wandb μ—°λ™ λ“±

## μ£Όμ” νμΌ/κµ¬μ„±
- adapter_model.safetensors : epoch 13 κΈ°μ¤€ νμΈνλ‹λ μ–΄λ‘ν„° κ°€μ¤‘μΉ
- adapter_config.json : PEFT/LoRA μ„¤μ • (target_modules, lora_alpha, r λ“±)
- tokenizer_config.json, tokenizer.json : ν† ν¬λ‚μ΄μ € μ •λ³΄
- trainer_state.json, training_args.bin : ν•™μµ μƒνƒ, ν•μ΄νΌνλΌλ―Έν„°
- README.md : μ²΄ν¬ν¬μΈνΈ μƒμ„Έ μ„¤λ…, μ‹¤ν— ν™κ²½, ν™μ©λ²•

## μ‹¤ν— ν™κ²½/ν•μ΄νΌνλΌλ―Έν„°
- base_model_name_or_path: unsloth/llama-3.2-1b-bnb-4bit
- lora_alpha: 16, r: 16, target_modules: [q_proj, k_proj, o_proj, gate_proj, v_proj, down_proj, up_proj]
- task_type: CAUSAL_LM
- inference_mode: true
- epoch: 13

## μ‹¤ν— κ²°κ³Ό/μ„±λ¥
- wandb/ : μ‹¤ν— λ΅κ·Έ, ν•μ΄νΌνλΌλ―Έν„°, μ„±λ¥ μ¶”μ΄, μ²΄ν¬ν¬μΈνΈ κ΄€λ¦¬
- trainer_state.json : ν•™μµ loss, ν‰κ°€ loss, μ„±λ¥ μ¶”μ΄ κΈ°λ΅

## μ¶”λ΅ /μ¬ν„ λ°©λ²•
```python
from transformers import pipeline
# LoRA/PEFT μ–΄λ‘ν„° μ μ© ν›„ pipeline μ¶”λ΅ 
```

## μ°Έκ³ /μ‹¤ν–‰ λ°©λ²•
- wandb/ μ°Έκ³  (μ‹¤ν— λ΅κ·Έ, μ„±λ¥ μ¶”μ΄)
- outputs/README.md, adapter_config.json μ°Έκ³ 