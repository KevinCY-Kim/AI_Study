# -*- coding: utf-8 -*-
"""llama파인튜닝과퓨샷프롬프트_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g7jStqRqdeSmIJwf0yg_1QodjfrGu4x4

본 교안은 테디노트와 위키독스를 참고하였습니다.

# llama 3.1 파인튜닝

WanDB API 필요: https://wandb.ai/site/ko/

https://github.com/unslothai/unsloth

## Unsloth: LLM을 파인튜닝 및 추론을 지원하는 라이브러리
"""

!pip install unsloth

!pip list

# Unsloth 라이브러리에서 FastLanguageModel을 임포트
from unsloth import FastLanguageModel
import torch

max_seq_length = 2048 # 최대 시퀀스 길이를 설정 ( 텍스트의 최대 길이를 지정)
dtype = None  # 자동 감지를 위해 None 설정. Tesla T4는 Float16, Ampere+는 Bfloat16 사용. 모델의 파라미터를 저장할 데이터 타입
load_in_4bit = True  # 메모리 사용량을 줄이기 위해 4비트 양자화 사용. 다만 양자화에 따른 손실이 있어서 필요에 따라 False로 설정 가능

# 4비트 사전 양자화된 모델 리스트 (더 빠른 다운로드 및 OOM 방지)
fourbit_models = [
    "unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama-3.1 8B 모델, 4비트 양자화
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
    "unsloth/Meta-Llama-3.1-70B-bnb-4bit",
    "unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # 405B 모델도 4비트 지원
    "unsloth/Mistral-Nemo-Base-2407-bnb-4bit", # Mistral 12B 모델, 4비트 지원
    "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit",
    "unsloth/mistral-7b-v0.3-bnb-4bit",        # Mistral v3 7B 모델, 4비트 지원
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3.5-mini-instruct",           # Phi-3.5 미니 인스트럭트 모델
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/gemma-2-9b-bnb-4bit",
    "unsloth/gemma-2-27b-bnb-4bit",            # Gemma 2.27B 모델, 4비트 지원
] # 더 많은 모델은 https://huggingface.co/unsloth 에서 확인 가능

# 사전 학습된 모델과 토크나이저 로드
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Meta-Llama-3.1-8B",  # 사용할 모델 이름
    max_seq_length = max_seq_length,         # 설정한 최대 시퀀스 길이
    dtype = dtype,                           # 데이터 타입 설정
    load_in_4bit = load_in_4bit,             # 4비트 양자화 여부
)

# PEFT(파라미터 효율적 파인튜닝) 모델 설정
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,  # LoRA 랭크 설정. 8, 16, 32, 64, 128 권장. r 값이 클수록 모델이 더 많은 정보를 학습할 수 있지만, 너무 크면 메모리를 많이 사용
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj"],  # PEFT 적용할 모듈 목록. 모델의 특정 부분(모듈)에만 학습
    lora_alpha = 16,        # LoRA 알파 설정 LoRA라는 기술이 얼마나 강하게 작용할지 조절
    lora_dropout = 0,       # LoRA 드롭아웃 설정. 0으로 최적화
    bias = "none",          # 바이어스 설정. "none"으로 최적화

    # "unsloth" 사용 시 VRAM 절약 및 배치 사이즈 2배 증가
    # 학습할 때 메모리를 절약하는 방법을 사용하는 설정
    use_gradient_checkpointing = "unsloth",  # 매우 긴 컨텍스트를 위해 "unsloth" 설정
    random_state = 3407,    # 랜덤 시드 설정
    use_rslora = False,     # 랭크 안정화 LoRA 사용 여부
    loftq_config = None,    # LoftQ 설정 (사용하지 않음)
)

# 모델에게 주어질 텍스트의 형식을 정의
alpaca_prompt = """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{}

### Response:
{}"""

# 맨윗줄: 모델에게 앞으로 제공될 지시사항을 기반으로 적절한 응답을 작성하라고 말함.
# 모델이 어떤 작업을 수행해야 하는지 명확히 이해할 수 있도록 도와줌
# Instruction:은 지시사항이 제공될 부분
# Response:는 모델이 생성해야 할 응답이 제공될 부분

# EOS 토큰 가져오기 (생성 종료를 위해 필요)
EOS_TOKEN = tokenizer.eos_token  # 반드시 EOS_TOKEN을 추가해야 함
EOS_TOKEN

# 문자열에 값을 넣는 방법
1. f-format

f"변수의 값은 = {변수}"

2. format 함수
"변수의 값은 ={}".format(변수)

3. 형식화된 출력
"변수의 값은 =%s"%(변수)

# 프롬프트 포맷팅 함수 정의
def formatting_prompts_func(examples):
    instructions = examples["instruction"]  # 데이터셋의 'instruction' 필드
    outputs      = examples["output"]       # 데이터셋의 'output' 필드
    texts = []
    for instruction, output in zip(instructions, outputs):
                                                           # EOS_TOKEN을 추가하지 않으면 생성이 무한히 계속됨
        text = alpaca_prompt.format(instruction, output) + EOS_TOKEN  # 프롬프트 형식에 맞게 텍스트 생성
        texts.append(text)
    return { "text" : texts, }  # 'text' 필드로 반환

from datasets import load_dataset  # Hugging Face datasets 라이브러리 임포트

# 데이터셋 로드 (Teddy Lee의 QA 데이터셋 미니 버전)
dataset = load_dataset("teddylee777/QA-Dataset-mini", split = "train")
dataset

dataset['instruction']

dataset['input']

dataset['output']

# 프롬프트 포맷팅 함수 적용하여 데이터셋 변환
dataset = dataset.map(formatting_prompts_func, batched = True,)


# 예제 엑셀 형태로 데이터셋을 만든다면?
# from datasets import load_dataset
# dataset = load_dataset( "csv", data_files = "data.csv", split = "train")
# dataset = dataset.map(formatting_prompts_func, batched=True)
#
#

dataset['text'][:5]

## 학습 설정
from trl import SFTTrainer  # TRL 라이브러리에서 SFTTrainer 임포트
from transformers import TrainingArguments  # 트랜스포머 라이브러리에서 TrainingArguments 임포트
from unsloth import is_bfloat16_supported  # BFloat16 지원 여부 확인 함수 임포트

# SFTTrainer 인스턴스 생성
trainer = SFTTrainer(
    model = model,                           # 학습할 모델
    tokenizer = tokenizer,                   # 사용할 토크나이저
    train_dataset = dataset,                 # 학습할 데이터셋 ★★★★★★★★
    dataset_text_field = "text",             # 데이터셋의 텍스트 필드 이름 ★★★★★★★★
    max_seq_length = max_seq_length,         # 최대 시퀀스 길이
    dataset_num_proc = 2,                    # 데이터셋 전처리에 사용할 프로세스 수 cpu
    packing = False,                         # 짧은 시퀀스의 경우 packing을 비활성화 (학습 속도 5배 향상 가능)
    args = TrainingArguments(
        per_device_train_batch_size = 2,     # 디바이스 당 배치 사이즈
        gradient_accumulation_steps = 4,     # 그래디언트 누적 단계 수
        warmup_steps = 5,                     # 워밍업 스텝 수
        # num_train_epochs = 1,               # 전체 학습 에폭 수 설정 가능
        max_steps = 60,                       # 최대 학습 스텝 수
        learning_rate = 2e-4,                 # 학습률
        fp16 = not is_bfloat16_supported(),   # BFloat16 지원 여부에 따라 FP16 사용
        bf16 = is_bfloat16_supported(),       # BFloat16 사용 여부
        logging_steps = 1,                    # 로깅 빈도
        optim = "adamw_8bit",                  # 옵티마이저 설정 (8비트 AdamW)
        weight_decay = 0.01,                  # 가중치 감쇠
        lr_scheduler_type = "linear",         # 학습률 스케줄러 타입
        seed = 3407,                           # 랜덤 시드 설정
        output_dir = "outputs",                # 출력 디렉토리
    ),
)

## 학습 실행
trainer_stats = trainer.train()  # 모델 학습 시작 # 3d6e4b9a0ddcd9edfafad59dcde0fd8c666954fd

# 모델 저장 로컬폴더에다가 저장하는 방식
model.save_pretrained("lora_model_0726")  # Local saving
tokenizer.save_pretrained("lora_model_0726")

# 이 이외에 허깅페이스나 다른 hub에 push해서 저장하는 방법이 있음
# 다만, 업로드 속도와 다운로드 속도를 고려해야함.

from unsloth import FastLanguageModel
import torch

# 저장된 경로 지정
save_directory = "lora_model_0726"

# 모델과 토크나이저 불러오기
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = save_directory,
    max_seq_length = 2048,
    dtype = None,
    load_in_4bit = True,  # 양자화 옵션을 동일하게 설정
)

# 추론해보기
FastLanguageModel.for_inference(model)  # 네이티브 2배 빠른 추론 활성화

# 추론을 위한 입력 준비
inputs = tokenizer(
[
    alpaca_prompt.format(
        "CES 2024의 주제에 대해서 말해줘.", # 인스트럭션 (명령어)
        "", # 출력 - 생성할 답변을 비워둠
    )
], return_tensors = "pt").to("cuda")  # 텐서를 PyTorch 형식으로 변환하고 GPU로 이동

from transformers import TextStreamer  # 텍스트 스트리밍을 위한 TextStreamer 임포트

text_streamer = TextStreamer(tokenizer)  # 토크나이저를 사용하여 스트리머 초기화

# 모델을 사용하여 텍스트 생성 및 스트리밍 출력
_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)  # 최대 128개의 새로운 토큰 생성

# 추론해보기
FastLanguageModel.for_inference(model)  # 네이티브 2배 빠른 추론 활성화

# 추론을 위한 입력 준비
inputs = tokenizer(
[
    alpaca_prompt.format(
        "CES 2024의 주제에 대해서 말해줘.", # 인스트럭션 (명령어)
        "", # 출력 - 생성할 답변을 비워둠
    )
], return_tensors = "pt").to("cuda")  # 텐서를 PyTorch 형식으로 변환하고 GPU로 이동

from transformers import TextStreamer  # 텍스트 스트리밍을 위한 TextStreamer 임포트

text_streamer = TextStreamer(tokenizer)  # 토크나이저를 사용하여 스트리머 초기화

# 모델을 사용하여 텍스트 생성 및 스트리밍 출력
_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)  # 최대 128개의 새로운 토큰 생성

# 추론해보기
FastLanguageModel.for_inference(model)  # 네이티브 2배 빠른 추론 활성화

# 추론을 위한 입력 준비
inputs = tokenizer(
[
    alpaca_prompt.format(
        "CES 2024의 주제에 대해서 말해줘.", # 인스트럭션 (명령어)
        "", # 출력 - 생성할 답변을 비워둠
    )
], return_tensors = "pt").to("cuda")  # 텐서를 PyTorch 형식으로 변환하고 GPU로 이동

from transformers import TextStreamer  # 텍스트 스트리밍을 위한 TextStreamer 임포트

text_streamer = TextStreamer(tokenizer)  # 토크나이저를 사용하여 스트리머 초기화

# 모델을 사용하여 텍스트 생성 및 스트리밍 출력
_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)  # 최대 128개의 새로운 토큰 생성

"""# 퓨샷 프롬프트:
모델에게 특정 작업을 수행하도록 지시할 때, 소수의 예시(보통 2~5개)를 포함하여 프롬프트를 구성하는 방식

# One-show 프롬프트
"""

# 원샷 프롬프트 정의
one_shot_prompt = """
### Instruction:
CES 2023의 주제에 대해서 말해줘.

### Response:
CES 2023의 주제: 혁신적인 기술과 미래 지향적인 제품들에 중점을 두었습니다.
특징: 인공지능, 사물인터넷, 그리고 지속 가능한 기술이 주요한 테마로 다뤄졌습니다.

### Instruction:
CES 2024의 주제에 대해서 말해줘.

### Response:
"""

# 추론을 위한 입력 준비
inputs = tokenizer(
    one_shot_prompt,
    return_tensors="pt"
).to("cuda")  # 텐서를 PyTorch 형식으로 변환하고 GPU로 이동

# TextStreamer 초기화
text_streamer = TextStreamer(tokenizer)

# 모델을 사용하여 텍스트 생성 및 스트리밍 출력
_ = model.generate(
    **inputs,
    streamer=text_streamer,
    max_new_tokens=128,
    do_sample=True,  # 샘플링을 활성화하여 다양성 부여 (옵션)
    temperature=0.7,  # 생성의 창의성 조절 (옵션)
    top_p=0.9  # 핵심 확률 질량을 설정하여 토큰 선택 (옵션)
)

"""# Two Shot 프롬프트"""

# 투샷 프롬프트 정의
two_shot_prompt = """
### Instruction:
CES 2022의 주제에 대해서 말해줘.

### Response:
예제1. CES 2022의 주제는 '기술과 혁신의 융합'으로,
특징: 5G, 자율주행, 그리고 스마트 홈 기술이 주요한 테마로 다뤄졌습니다.

### Instruction:
CES 2023의 주제에 대해서 말해줘.

### Response:
예제2. CES 2023의 주제는 혁신적인 기술과 미래 지향적인 제품들에 중점을 두었습니다.
특징: 인공지능, 사물인터넷, 그리고 지속 가능한 기술이 주요한 테마로 다뤄졌습니다.

### Instruction:
CES 2024의 주제에 대해서 말해줘.

### Response:
"""

# 추론을 위한 입력 준비
inputs = tokenizer(
    two_shot_prompt,
    return_tensors="pt"
).to("cuda")  # 텐서를 PyTorch 형식으로 변환하고 GPU로 이동

# TextStreamer 초기화
text_streamer = TextStreamer(tokenizer)

# 모델을 사용하여 텍스트 생성 및 스트리밍 출력
_ = model.generate(
    **inputs,
    streamer=text_streamer,
    max_new_tokens=128,
    do_sample=True,       # 샘플링을 활성화하여 다양성 부여 (옵션)
    temperature=0.7,      # 생성의 창의성 조절 (옵션)
    top_p=0.9              # 핵심 확률 질량을 설정하여 토큰 선택 (옵션)
)

# 랭체인: lib,
                    # langchain-community: 애플리케이션을 개발할때 쓰이는 각종 기능들을 담고 있다,
                    # ollama: lamma3나 다른 llm들을 다운로드 받고 쓸 수 있는 라이브러리
                    # chromadb: 청킹 vector들을 저장하는 sql 종류
                    # PyPDF2: pdf에서 글자를 추출해주는 라이브러리 이거 말고도 OCR로도 추출 가능(클로바ocr, 구글ocr, AI ocr모델 등)
!pip install --upgrade langchain langchain-community ollama chromadb PyPDF2

# 웹 방식
from langchain_community.document_loaders import WebBaseLoader

# 위키피디아 정책과 지침
url = 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EC%A0%95%EC%B1%85%EA%B3%BC_%EC%A7%80%EC%B9%A8'
loader = WebBaseLoader(url) # url을 셋팅

# 웹페이지 텍스트 -> Documents
docs = loader.load() # load 내장함수로 내용들을 불러옴

# 내용출력
print(len(docs)) # 문서의 길이
print(len(docs[0].page_content)) # 첫번째 문서의 내용
print(docs[0].page_content[5000:6000]) # 첫번째 문서에서 5000: 6000 구간슬라이싱


# Text Split (Documents -> small chunks: Documents)
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 청킹 작업                           # 문장 토큰을 1000길이로 짜름(청킹한다라는뜻), overlap은 끝에 몇글자가 중복되도록 짜를거냐
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs) # 스플리터로 docs 들을 청킹함

print(len(splits))
print(splits[10])


# PDF 파일 방식
# uploaded = files.upload()
# pdf_file = next(iter(uploaded))
# print(f"업로드된 파일: {pdf_file}")

# # PDF 텍스트 추출 및 분할
# try:
#     loader = PyPDFLoader(pdf_file)
#     docs = loader.load()
# except Exception as e:
#     print(f"PDF 로딩 중 오류 발생: {e}")
#     exit()

# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
# splits = text_splitter.split_documents(docs)

# page_content 속성
splits[10].page_content

splits[10].metadata

!pip install langchain-openai

# import os
# os.environ["OPENAI_API_KEY"] = "API키"

# Indexing (Texts -> Embedding -> Store)
from langchain_community.vectorstores import Chroma
#from langchain_openai import OpenAIEmbeddings
from langchain.embeddings import HuggingFaceEmbeddings

# 무료 임베딩 모델을 사용하여 임베딩 생성 (모델의 임베딩 성능에 따라 검색능력이 많이 좌우 됨.)
# Chroma 벡터 스토어 구축
embeddings = HuggingFaceEmbeddings(model_name="all-mpnet-base-v2")
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="chroma_store_new_v2" # 벡터스토어를 저장할 폴더이름 중복되면 안된다.
)

# 유료임베딩
# vectorstore = Chroma.from_documents(documents=splits,
#                                     embedding=OpenAIEmbeddings())

docs = vectorstore.similarity_search("집행에 대해서 설명해주세요.")
print(len(docs))
print(docs[0].page_content)

!pip install colab-xterm

# Commented out IPython magic to ensure Python compatibility.
# %load_ext colabxterm

# Commented out IPython magic to ensure Python compatibility.
# %xterm

# 아래를 한줄씩 마우스 오른쪽 붙여넣기로
# curl -fsSL https://ollama.com/install.sh | sh
# ollama serve & ollama pull llama3 & ollama pull nomic-embed-text

import ollama
from langchain.llms.base import LLM
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from typing import Optional


# Prompt 템플릿 정의
template = """Answer the question based only on the following context:
{context}

Question: {question}
"""

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=template,
)

# 7. Ollama Llama3 모델 설정
# Ollama의 LLM을 LangChain과 통합하기 위해 커스텀 LLM 클래스를 정의합니다.
from langchain.llms.base import LLM
from typing import Optional

class OllamaLLM(LLM):
    model_name: str = "llama3"
    temperature: float = 0.0

    def _call(self, prompt: str, stop: Optional[list] = None) -> str:
        response = ollama.chat(model=self.model_name, messages=[{'role': 'user', 'content': prompt}])
        return response['message']['content']

    @property
    def _llm_type(self):
        return "ollama"

# LLM 인스턴스 생성
llm = OllamaLLM(model_name="llama3", temperature=0.0) # temperature 가 작으면 모델의 대답 다양성이 적고, 높으면 대답을 다양하게 내뱉음


# Retriever 설정
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})  # 상위 5개의 관련 문서 검색

# LLMChain 설정
llm_chain = LLMChain(prompt=prompt, llm=llm)


def rag_chain(question):
    # 관련 문서 검색
    retrieved_docs = retriever.get_relevant_documents(question)

    # 관련있는 문서의 내용을 뉴라인 기준으로 concatenate 함 이걸 모델한테 context로 전달해서 답변을 가져올 것이다.
    formatted_context = "\n\n".join(doc.page_content for doc in retrieved_docs)


    # Prompt에 질문과 컨텍스트를 전달하여 답변 생성
    answer = llm_chain.run({"context": formatted_context, "question": question})
    return answer

# 인터랙티브하게 질문하고 응답 받기
print("질문을 입력하세요. 종료하려면 'exit'을 입력하세요.")

while True:
    question = input("질문: ")
    if question.lower() == 'exit':
        print("프로그램을 종료합니다.")
        break
    answer = rag_chain(question)
    print(f"답변: {answer}\n")

# RAG 코드를 쓰고 싶다면,

# 임베딩 모델 선정
# llm 모델 선정
# PDF 만들기